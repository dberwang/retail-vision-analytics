# Retail Vision Analytics

## 1. Project Proposal

Millions of brick-and-mortar stores have cameras; however, most retailers only use their footage for security. Video of your customers is a data treasure trove! So why aren't retailers analyzing their video footage? Because manually reviewing videos is costly and challenging. 

To succeed in a highly competitive marketplace, retailers must know their customers. The capability to produce customer analytics would help businesses better serve their customers and gain an edge over the competition.

The primary questions that I seek to answer are:

* Can useful customer-specific analytics be generated from video using object detection and tracking in Computer Vision?
* How reliable and costly is an automated process?

Retail Vision Analytics for retail wasn't feasible ten years ago, but it may be possible today. 

## 2. Data Collection

The primary video data used was the Ground VIRAT Video Dataset Release 2.0. This publicly available video is similar to footage that a retailer would have of customers walking in parking lots and walking areas outside of its stores. 

The camera views are from various fixed locations, outdoors and during daylight hours. These videos capture human activity and are a recognized benchmark dataset for the computer vision community.

## 3. Data Wrangling

The VIRAT videos include annotation files that notate bounding boxes for known objects in each frame. I planned to use these annotations as the ground truth for measuring the performance of my model. While conducting a visual review of the annotation data, I found numerous discrepancies. This notebook details the videos and several annotation issues discovered during a manual inspection.  

Next, I decided to test the performance of my models with a small, accurate annotation dataset on one video and perform a visual inspection for the others. Using Roboflow, I annotated the first portion of one VIRAT video. 

These annotations will be the ground truth for computing our accuracy metrics. Due to time and resource limitations for annotating video, the accuracy of the other video footage was by manual visual inspection.

## 4. Models

Our prototype app must do the following:

* Detect and locate persons in the video.
* Assign each detected person a unique id and track them through the video.
* Generate analytics.  

## 4.1 YOLOv7 with DeepSORT

My first solution was to track objects using DeepSORT on persons detected by You-Only-Look-Once v7 (YOLOv7). YOLOv7 to detect and record the location of the class person. YOLOv7 is a state-of-the-art object detection algorithm that uses a convolutional neural network (CNN) trained on the MS COCO dataset. Locations data is a bounding box around each detected person for each video frame, but this data does not track persons from one frame to the next.

YOLOv7 is a state-of-the-art object detection algorithm that uses a convolutional neural network (CNN) and includes models pre-trained on the MS COCO dataset and the source code. I considered transfer learning to improve the pre-trained model. 

After evaluating the pre-trained model and concluding that it is very good at detecting people, I decided that transfer learning was unnecessary.

The next step was object tracking using DeepSORT (Simple Online Realtime Tracking) algorithm. DeepSORT assigns an ID to each object and tracks objects using motion and appearance information. Unfortunately, DeepSORT had too many ID switches and frequently failed to handle occlusions involving groups of people and shaded areas. This algorithm's performance on our test video was unacceptable.

## 4.2 YOLOX with ByteTrack

The second object-tracking algorithm that I evaluated was ByteTrack. ByteTrack uses an association method that does not discard low-score boxes and employs tracklets to recover actual objects and filter out background detections. ByteTrack's pre-trained model uses YOLOX for detection.

ByteTrack had extremely few ID switches in the test video despite numerous occlusions and shady regions. This algorithm's performance was acceptable and made very few errors. 

Using the ground truth generated by manual annotations, I scored the test video using TrackEval. TrackEval provides several tracking evaluation metrics, including HOTA and other tracking benchmarks.

## 5> The Prototype

